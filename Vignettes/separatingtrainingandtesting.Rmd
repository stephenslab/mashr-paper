---
title: "SampleRunVignette"
output: html_document
---

First, we want to load the code:

```{r}
devtools::install_github("surbut/matrix_ash")
library("mash")
devtools::load_all(".")

source("~/matrix_ash/R/main.R")
source("~/matrix_ash/R/mixEm.R")

library("bigmemory")
library("SQUAREM")
library("mvtnorm")

```

Then, we need to load our matrix of 'strong' t statistics to create the covariance matrices as well as the matrix of factors from our SFA model. In this example, we will not use the additional BMA configurations (i.e., bma=FALSE). Also load the maximum beta hats and t statistics for grid selection.

```{r}

set.seed(123)
setwd("~/Dropbox/test/")
R=5


covdat = factor_sim(n=1000,K=10,betasd=1,esd=0.3,d=5)
b.gp.hat=covdat$betahat
se.gp.hat=covdat$sebetahat
t.stat=covdat$tstat


R=ncol(b.gp.hat)#number of tissues
X.t=as.matrix(t.stat)
#X.real=X.t[which(truth$config!=0),]
X.real=X.t
X.c=apply(X.real,2,function(x) x-mean(x)) ##Column centered matrix of t statistics
R=ncol(X.t)
M=nrow(X.c)




lambda=covdat$lambda
factors=covdat$factors



covmat=compute.covmat(b.gp.hat,se.gp.hat,Q=5,X.c,lambda,P=2,A="singlerun",factor.mat=factors,bma=FALSE)

K=length(covmat)
K
covmat[[K]]
```

Now, we let's remove these strong tstats and split simulate new data into test and training. This will compute the HM weights on training data and return a file of likelihoods for the training data. I use my factor simulator to simulate true betas and betahats. 

```{r}
rm(b.gp.hat)
rm(se.gp.hat)

newdat = factor_sim(n=10000,K=10,betasd=0.5,esd=0.3,d=5)
b.hat=newdat$betahat
se.hat=newdat$sebetahat


J=nrow(b.hat)

b.train=b.hat[1:1000,]
se.train=se.hat[1:1000,]

compute.hm.train(train.b = b.train,se.train = se.train,covmat = covmat,A="filename") ##compute the HM weights on training data

```
Note creation of three files: pis, a pdf of pis, and a matrix  of likelihoods of training data.

Now,we remove the training data and create our test data.

```{r}
rm(b.train)
rm(se.train)

b.test=b.hat[1001:nrow(b.hat),]
se.test=se.hat[1001:nrow(se.hat),]
b.truth.test=newdat$beta[1001:nrow(b.hat),]
````

We read in the previously computed pis and the covariane matrix and compute the test likelihood and corresponding weights. Note the return of a file of test likeilihoods and posterior weights. Here, I will only do the computation for 10 gene-snp pairs. 

```{r}
A="filename"
pis=readRDS(paste0("pis",A,".rds"))$pihat

barplot(t(as.matrix(pis)))
(covmat[[which.max(pis)]])


compute.lik.test(b.gp.hat = b.test,J = 100,se.gp.hat = se.test,covmat,A="filename",pis) 
```

We could stop here, but we might also want to compute the posterior means for each componenet and the weighted 'grand mean', as well as the weighted tail probabilities. The componenet specific quanitites are computed in 'compute.mix.test' and the weighted quantities in 'test.quant'. We can choose to save the rds object if 'save = TRUE' but this consumes a lot of space.

```{r}
all.arrays=compute.mix.test(b.gp.hat = b.test,J = 100,se.gp.hat = se.test,covmat = covmat,A="filename",save=FALSE)##componenet specific posterior means, variances, tail probabilities
test.quant(A="filename",all.arrays)##weighted posterior quantities
```


You can test to make sure the array computations are accurate for a particular snp. First, we check the component specific snp and then we check the weighting.

```{r}
##test for snp 1##
j=1
b.mle=as.vector(t(b.test[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se.test[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)
k=10
U.gp1kl <- (post.b.gpkl.cov(V.gp.hat.inv, covmat[[k]]))
(mu.gp1kl <- as.array(post.b.gpkl.mean(b.mle, V.gp.hat.inv, U.gp1kl)))

all.arrays$post.means[j,k,]

A="filename"
post.means=read.table(paste0("post.mean.",A,".txt"))
post.weights=readRDS("post.weight.filename.rds")

(post.weights[j,]%*%all.arrays$post.means[j,,])
(post.means[j,])

lapply(seq(1:R),function(r){plot(b.truth.test[1:100,r],post.means[,r])})
