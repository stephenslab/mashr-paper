---
title: "PerSnpVignette"
output: html_document
---

Here, we show how we can compute the prior weights on a large number of gene snp pairs and compute the weighted quanities one snp at a time.
```{r,echo=FALSE}
devtools::install_github("surbut/matrix_ash")
library("mash")
library("bigmemory")
library("SQUAREM")
library("mvtnorm")

suppressPackageStartupMessages(require("mash"))
suppressPackageStartupMessages(require("bigmemory"))
suppressPackageStartupMessages(require("SQUAREM"))
suppressPackageStartupMessages(require("mvtnorm"))
```

Then, we need to load our matrix of 'strong' t statistics to create the covariance matrices as well as the matrix of factors from our SFA model. In this example, we will not use the additional BMA configurations (i.e., bma=FALSE). Also load the maximum beta hats and t statistics for grid selection.

```{r}
set.seed(123)
R=5
covdat = factor_sim(J=1000,K=10,betasd=1,esd=0.1,d=5)
b.gp.hat=covdat$betahat
se.gp.hat=covdat$sebetahat
t.stat=covdat$tstat

lambda.mat=covdat$lambda
factor.mat=covdat$factors
covmat=compute.covmat(b.gp.hat,se.gp.hat,Q=5,t.stat,lambda.mat=lambda.mat,P=2,A="simulation",factor.mat=factor.mat,bma=TRUE)

K=length(covmat)
K
covmat[[K]]

```
Now, we let's remove these strong tstats and split simulate new data into test and training. This will compute the HM weights on training data and return a file of likelihoods for the training data. I use my factor simulator to simulate true betas and betahats. Note that we want only the rownames to include meta data (and not an additional column) because this will impace the dimension compatibility of the prior covariance matrices and V.

```{r}
rm(b.gp.hat)
rm(se.gp.hat)

newdat = factor_sim(J=10000,K=10,betasd=1,esd=0.1,d=5)
b.train=newdat$betahat
se.train=newdat$sebetahat


##Not 
indices=seq(1,nrow(b.hat))
rownames(b.hat)=paste("gene_snp",indices,sep="")##now, we want our data frame to have rownames

J=nrow(b.hat)
head(b.hat)
```

Now, we separate into training data. I will train on 10000 gene snp Pairs, which tra

```{r,eval=FALSE}
compute.hm.train(train.b = b.train,se.train = se.train,covmat = covmat,A="Simulations")
##compute the HM weights on training data
##check to show that the likelihood computatations are correct
```

```{r}
A="Simulations"
likmat=readRDS(paste0("liketrain",A,".rds"))
likmat[1,1:10]
lik.func(b.mle = b.train[1,],V.gp.hat = diag(se.train[1,])^2,covmat)[1:10]
```
Note creation of three files: pis, a pdf of pis, and a matrix  of likelihoods of training data.
Now,we remove the training data and create our test data. We can see that it contains the gene snp pairs in the rows and the standardized effect across subgroups in the columns.

```{r,eval=FALSE}
rm(b.train)
rm(se.train)

testdat = factor_sim(J=10000,K=10,betasd=1,esd=0.1,d=5)
b.test=testdat$betahat
se.test=testdat$sebetahat
b.truth.test=testdat$beta
````

We read in the previously computed pis and the covariane matrix and compute the test likelihood and corresponding weights. Note the return of a file of test likeilihoods and posterior weights. Here, I will only do the computation for 100 gene-snp pairs. 

```{r}
A="Simulations"
pis=readRDS(paste0("pis",A,".rds"))$pihat

barplot(t(as.matrix(pis)))
(covmat[[which.max(pis)]])
```

Now, we can compute the posterior quanitites for each gene snp pair. Here, we'll test it on 500 gene snp pairs.

```{r,eval=FALSE}

weightedquants=lapply(seq(1:500),function(j){total.quant.per.snp(j,covmat,b.gp.hat=b.test,se.gp.hat = se.test,pis,A="Simulations",checkpoint = FALSE)})
```

Note that we create a file of posterior weighted means, lfsr and marginal variances.
       
       
To make sure that these computations are correct,

```{r}
j=sample.int(100,1)
all.arrays=post.array.per.snp(j=j,covmat = covmat,b.gp.hat = b.test,se.gp.hat = se.test)

b.mle=as.vector(t(b.test[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se.test[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)
k=17

U.gp1kl <- (post.b.gpkl.cov(V.gp.hat.inv, covmat[[k]]))
(mu.gp1kl <- as.array(post.b.gpkl.mean(b.mle, V.gp.hat.inv, U.gp1kl)))
(all.arrays$post.means[k,])
plot(all.arrays$post.means[k,],mu.gp1kl)

##Now, check to make sure weighting is correct
post.means=as.matrix(read.table(paste0(A,"posterior.means.txt"))[,-1])
post.weights=as.matrix(read.table(paste0(A,"post.weights.txt"))[,-1])

plot(post.means[j,],post.weights[j,]%*%all.arrays$post.means)
(post.means[j,])
(post.weights[j,]%*%all.arrays$post.means)

```

And observe the beautiful shrinkage of the null values!! 
```{r,shrinkage}
plot(b.test[1:500,],post.means,xlim=c(-1,1),ylim=c(-1,1))
nulls=which(b.truth.test[,1]=="0",)
null.vals=intersect(1:nrow(post.means),nulls)
points(b.test[null.vals,],post.means[null.vals,],col="red")
```

And the nearly perfect capturing of truth!
```{r}
plot(b.truth.test[1:500,],post.means,main="PosteriorMean vs True Effects")
```
