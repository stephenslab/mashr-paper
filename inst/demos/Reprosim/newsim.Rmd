---
title: "Reproducible Simulation"
output: html_document
---

First, install the mashr package. You will also need to install the R packages
ExtremeDeconvolution, gplots, ashr and colorRamps. Also install the command
line program sfa for sparse factor analysis.


```{r}
knitr::opts_chunk$set(cache=FALSE)
library(mashr)
library(gplots)
library(colorRamps)

# Only needed for model fitting ("fit_model" chunk).
library(ashr)

# Only needed for optional "generate_covmatswithbovy" chunk.
library(ExtremeDeconvolution)
```


I simulated the data this way
```{r, eval=FALSE}
set.seed(123)

## Simulate 20000 gene snp pairs with 400 true effects
data=factor_sim_new(J = 20000)
saveRDS(data,"newsimdata.rds")
````

Here, I load my previously simulated data:

```{r,echo=T}
sim_data=readRDS(unzip(zipfile="newsimdata.rds.zip"))
t         = sim_data$tstat
bhat      = sim_data$betahat
sebetahat = sim_data$sebetahat
beta      = sim_data$beta;s.j=matrix(rep(1,ncol(t)*nrow(t)),ncol=ncol(t),nrow=nrow(t))
```

In this step, we run ash to choose the effects that satisfy a univariate ash lfsr threshold of 0.05 in at least one tissue. Again, you can choose to excecute this step yourself, but for your convenience, I've included these files in the directory and zipped them.

```{r,eval=FALSE}
t.fit  = sim_data$tstat
se.fit = sim_data$sebetahat / sim_data$sebetahat

univariate.ash.pm=matrix(nrow=nrow(t.fit),ncol=ncol(t.fit))
univariate.ash.lfsr=matrix(nrow=nrow(t.fit),ncol=ncol(t.fit))

cat("Fit ash models separately for each tissue.\n")
for(x in 1:44){
  cat("Fitting ash model for tissue",x,"\n")
  b=ash(betahat=t.fit[,x],sebetahat=se.fit[,x],mixcompdist="normal")
  univariate.ash.pm[,x]   = b$result$PosteriorMean
  univariate.ash.lfsr[,x] = b$result$lfsr
}

write.table(univariate.ash.lfsr,"univariate.ash.lfsrnewdata.txt")
write.table(univariate.ash.pm,"univariate.ash.pmnewdata.txt")
lfsr=read.table(unzip(zipfile="univariate.ash.lfsrnewdata.txt.zip"))
index=which(rowSums(lfsr<0.05)>0)
write.table(t[index,],"maxtsim.txt",row.names = F,col.names = F)
```



Then we perform SFA on the maxes, however we choose to select them. I chose those which satisfied an LFSR threshold of 0.05 in at least one subgroup, and I saved the factors and loadings for your convenience in the directory as you can see. However, you can reperform yourslef with the SFA software installed. 

See [link](..\SFA\running_with_sfa.html) to a vignette here.

```{r sfa, eval=FALSE}
wc -l maxtsim.txt
##here there were 269 rows, so -g is 269
sfa -gen ./maxtsim.txt -g 269 -n 44 -o reproseed i -k 5
```

Then we load the loadings and factors and compute covariance matrices, train the model, and compute posteriors:

```{r generate covmats,eval=FALSE}
factor.mat=as.matrix(read.table("reproseed_F.out"))
lambda.mat=as.matrix(read.table("reproseed_lambda.out"))
s.j=matrix(rep(1,ncol(t)*nrow(t)),ncol=ncol(t),nrow=nrow(t))

A="reproseedomega2"
cov=compute.covmat(b.gp.hat = t,sebetahat = s.j,Q =5, t.stat=t[index,],lambda.mat=lambda.mat,P=3,A=A, factor.mat = factor.mat,bma = TRUE,zero=TRUE,power = 2)$cov

out <- compute.hm.train.log.lik.pen(train.b = t,se.train = s.j,covmat = cov,A=A,pen=1)

pis=readRDS(paste0("pis",A,".rds"))$pihat
b.test=t[1:20000,]
se.test=s.j[1:20000,]
weightedquants=lapply(seq(1:20000),function(j){total.quant.per.snp(j,cov,b.gp.hat=b.test,se.gp.hat = se.test,pis,A=A,checkpoint = FALSE)})
```

If we wanted to use the Extreme Deconvolution package, we could add an additional step. However, in this vignette we use the code above (without Extreme Deconvolution) to generate posteriors. 

``````{r generate covmatswithbovy,eval=FALSE}
ms=deconvolution.em.with.bovy(t.stat=t[index,],factor.mat,s.j,lambda.mat,K=3,P=3)
saveRDS(ms,"mstruecovs.rds")
A="reproseedomegawithED"
coved=compute.hm.covmat.all.max.step(max.step = ms,b.gp.hat = t,sebetahat = s.j,Q =5, t.stat=t[index,],lambda.mat=lambda.mat,P=3,A=A, factor.mat = factor.mat,bma = TRUE,zero=TRUE,power = 2)$cov
```

You can look at the output of `ms$true.covs` to see how these estimates compare with the original covariance matrix, rank 5 SFA and rank 3 SVD decomposition respectively.

##Posteriors

After computing the posteriors, you should have files corresponding to the $\pi$ hierarchical weights and posterior quantities with the prefix you designated in A. 
I've included my computations in this directory using the prefix "reproseedomega2" for you to check your results. 

Now let's check the results:
```{r}
mash.means=read.table(unzip(zipfile="reproseedomega2posterior.means.txt.zip"))[,-1]
univariate.ash.pm=read.table(unzip(zipfile="univariateashpm.txt.zip"))
lfsr.mash=read.table(unzip(zipfile="reproseedomega2lfsr.txt.zip"))[,-1]
lfsr.ash=read.table(unzip(zipfile="univariate.ash.lfsrnewdata.txt.zip"))

sebetahat = sim_data$sebetahat
sebetahat = sebetahat[1:20000,]
standard  = sqrt(mean((beta[1:20000,]-bhat[1:20000,])^2))
sqrt(mean((beta[1:20000,]-mash.means[1:20000,]*sebetahat)^2))/standard
sqrt(mean((beta[1:20000,]-univariate.ash.pm[1:20000,]*sebetahat)^2))/standard

```


```{r echo=FALSE,cache=TRUE}
beta=as.matrix(beta[1:20000,])
lfsr.mash=as.matrix(lfsr.mash)
lfsr.ash=as.matrix(lfsr.ash)

mash.means=as.matrix(mash.means)
ash.means=as.matrix(univariate.ash.pm)
```

Then, we can ask how many truly non-zero associations we capture at a given threshold.

```{r echo=TRUE}
thresh=0.05


sum(sum(beta!=0&lfsr.mash<thresh))/sum(beta!=0)
sum(sum(beta!=0&lfsr.ash<thresh))/sum(beta!=0)

```

Then, we compute the nominal lfsr (S) value, and we ask how many we correctly and incorrectly sign at a given threshold.

```{r echo=TRUE}


sign.test.mash=beta*mash.means
sign.test.ash=beta*ash.means
###compute nominal lfsr threshold
mean(lfsr.mash[lfsr.mash<0.05])
mean(lfsr.ash[lfsr.ash<0.05])


##ask what proportion of associations do we correctly sign 
sum(sign.test.mash>0&lfsr.mash<thresh)/sum(beta!=0)
sum(sign.test.ash>0&lfsr.ash<thresh)/sum(beta!=0)

```

And here we plot a power comparison:

```{r roccurve1shared,echo=FALSE}
mash.power=NULL
ash.power=NULL



mash.fp=NULL
ash.fp=NULL
bma.fp=NULL



thresholds=seq(0.01,1,by=0.01)
for(s in 1:length(thresholds)){
thresh=thresholds[s]

##sign power is the proportion of true effects correctly signed at a given threshold
mash.power[s]=sum(sign.test.mash>0&lfsr.mash<=thresh)/sum(beta!=0)
ash.power[s]=sum(sign.test.ash>0&lfsr.ash<=thresh)/sum(beta!=0)




##false positives is the proportion of null effects called at a given threshold
mash.fp[s]=sum(beta==0&lfsr.mash<=thresh)/sum(beta==0)
ash.fp[s]=sum(beta==0&lfsr.ash<=thresh)/sum(beta==0)






}

#frow=c(1,2))

plot(mash.fp,mash.power,cex=0.5,pch=1,xlim=c(0,0.2),lwd=1,ylim=c(0,1),col="green",ylab="True Positive Rate",xlab="False Positive Rate",type="l")
title("True Positive vs False Positive",cex.main=1.5)
lines(ash.fp,ash.power,cex=0.5,pch=1,ylim=c(0,1),col="red")
legend("bottomright",legend = c("mash","ash"),col=c("green","red"),pch=c(1,1))

```

To show our calculations are correct, we show the calculation for a sample pair.

````{r}
j=sample.int(100,1)

A="reproseedomega2"

pis=readRDS(paste0("pis",A,".rds"))$pihat
b.test=t[1:20000,]
se.test=s.j[1:20000,]

covmat=readRDS(paste0("covmat",A,".rds"))
all.arrays=post.array.per.snp(j=j,covmat = covmat,b.gp.hat = t,se.gp.hat = s.j)

b.mle=as.vector(t(t[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(s.j[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)
k=3

U.gp1kl <- (post.b.gpkl.cov(V.gp.hat.inv, covmat[[k]]))
mu.gp1kl <- as.array(post.b.gpkl.mean(b.mle, V.gp.hat.inv, U.gp1kl))
#(all.arrays$post.means[k,])
all.equal(as.numeric(all.arrays$post.means[k,]),as.numeric(mu.gp1kl))

```

Now, check to make sure weighting is correct

```{r}
pm=paste0(A,"posterior.means.txt.zip")
post.means=as.matrix(read.table(unzip(pm)))[,-1]
lik.snp=lik.func(b.mle,V.gp.hat,covmat)
post.weights=t(lik.snp*pis/sum(lik.snp*pis))

all.equal(as.numeric(post.means[j,]),as.numeric(post.weights%*%all.arrays$post.means))

```

In this chunk, we show a heatmap of some of the major patterns of sharing (the UKs) that are *learned* from the data, equivalent to our supplemental figure.

```{r,eval=T}
A="reproseedomega2"

pis=readRDS(paste0("pis",A,".rds"))$pihat

pi.mat=matrix(pis[-length(pis)],byrow = T,ncol=54)
cov=readRDS(paste0("covmat",A,".rds"))

mostimportant=c(2:9)
for(k in mostimportant){
  x=cov[[k]]/max(diag(cov[[k]]))

  heatmap.2(x,col = blue2green(256),dendrogram="none",density="none",trace="none",main=paste0("HeatmapofUk",k))
v=svd(cov[[k]])$v
barplot(v[,1]/max(v[,1]),
         xlab = paste0("EigenVector1 of UK",k))
}

